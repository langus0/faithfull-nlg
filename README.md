
# Prepare environment

Install dependencies using [uv package manager](https://docs.astral.sh/uv/getting-started/installation/):
```sh
uv sync
```

Model config files can be found under `src/configs/models`, datasets in `data/meta_eval`, and specific aspects for each dataset in `src/configs/eval_aspects`

To reproduce the using with Ollama, install the package following the instructions in the [Ollama repository](https://github.com/ollama/ollama). After installation, you can pull the required model using the following command:

```sh
ollama pull nemotron:70b-instruct-q8_0
```
Then create the model from the modelfile. Example for nemotron:
```sh
ollama create eval_nemo -f src/configs/models/modelfile_nemo
```

# Run evaluation with modificaiton

## Run evaluation and modify the results

Open the script `run_eval_all_severities.sh` and adjust the parameters to your needs. The script will run the evaluation using the OpenNLG model and then apply the error severity modifications.

```bash
sh run_eval_all_severities.sh
```

The specific modifications are defined in `src/mods.py` and are applied in `src/eval_mod.py`. 

To test a specific modification (int or text) outside the whole loop, you can use the `run_eval_severity.sh` script.

## Use previously generated evaluation content

To speed up the evaluation of different modification scenarios once a modification was run, the OpenNLG evaluation results can be copied to a "pregen" file based on the already generated evaluation, to be reused for further modifications. The script `src/copy_results_to_pregen.py` can be used for that, of which example of usage can be found in the `run_eval_all_severities.sh` script.

The pregen file will by default include the result of the modification (under the key `result_modified`) so that another modification can be added on top of it. It can be excluded out of the file generation using the flag `--exclude-premodified-result`.

Once the pregen json file is created, you can link it in the `--data` parameter of the eval .py scripts instead of the original dataset json file, and the evaluation will use the pre-generated content instead of running the OpenNLG model again. In eval scripts, the flag `--use-premodified-result` can be used to indicate that also the result of the previous modification (described in the paragraph above) should be used (so that you can stack this mod on top of the previous mod).

## Measure impact of error severity modifications on the OpenNLG results

Use the script `run_eval_impact.sh` to perform incremental severity modification on each one error of the evaluation, by which you can measure the error's impact on the evaluation's overall score.

``bash
sh run_eval_impact.sh
``

Note that this script only measures the int severity increment, defined in a different method than the original modification (all of them can be found in `mods.py`)

# Evaluate results

## Calculate metrics

To count the changes in overall scores previously generated by `run_eval_severities.sh` and calculate metrics use the following script:
```sh
sh run_calculate_metrics.sh
```

The result will be saved to a file `scores_summary.json` in your result directories (summaries are visible in the github repo).

Single use example:
```sh
uv run python src/calculate_error.py --results-dir results/eval_mod_results/qags/factual_consistency/eval_nemo_textsev1
```

## Inspect a specific generation & modification

To pretty print a specific data point from evaluation results, use `show_result.py`. Use example:
```sh
uv run python src/show_result.py --json_file results/eval_mod_results/eval_nemo_textsev1/cnndm-79.json
```